@misc{allianz,
  organization = {Allianz Global Corporate \& Specialty},
  title        = {Safety and Shipping Review},
  year         = {2023},
  month        = {May},
  pages        = {4},
  url          = {https://commercial.allianz.com/news-and-insights/reports/shipping-safety.html},
  note         = {Accessed: 2024-04-19}
}

@misc{who,
  organization = {World Health Organization},
  title        = {Drowning},
  year         = {2023},
  month        = {July},
  day          = {25},
  url          = {https://www.who.int/news-room/fact-sheets/detail/drowning},
  note         = {Accessed: 2024-04-24}
}

@inbook{iamsar,
  title        = {Chapter 5. Search techniques and operations},
  booktitle    = {International Aeronautical and Maritime Search and Rescue Manual},
  organization = {International Maritime Organization and International Civil Aviation Organization},
  year         = {2022},
  volume       = {II},
  chapter      = {5},
  isbn         = {9789280117356},
  url          = {https://store.icao.int/en/international-aeronautical-and-maritime-search-and-rescue-manual-volume-ii-mission-co-ordination-doc-9731-2}
}

@article{trummel1986,
  title={The complexity of the optimal searcher path problem},
  author={Trummel, KE and Weisinger, JR},
  journal={Operations Research},
  volume={34},
  number={2},
  pages={324--327},
  year={1986},
  publisher={INFORMS}
}

@inproceedings{terry2021pettingzoo,
 author      = {Terry, J and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario  and Hari, Ananth  and Sullivan, Ryan and Santos, Luis S and Dieffendahl, Clemens and Horsch, Caroline and Perez-Vicente, Rodrigo and Williams, Niall  and Lokesh, Yashas  and Ravi , Praveen },
 booktitle   = {Advances in Neural Information Processing Systems},
 editor      = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages       = {15032--15043},
 publisher   = {Curran Associates, Inc.},
 title       = {PettingZoo: Gym for Multi-Agent Reinforcement Learning},
 url         = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7ed2d3454c5eea71148b11d0c25104ff-Paper.pdf},
 volume      = {34},
 year        = {2021}
}

@software{Terry_PettingZoo_Gym_for,
  author  = {Terry, Jordan and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario and Hari, Ananth and Sullivan, Ryan and Santos, Luis and Perez, Rodrigo and Horsch, Caroline and Dieffendahl, Clemens and Williams, Niall and Lokesh, Yashas},
  license = {MIT},
  title   = {{PettingZoo: Gym for multi-agent reinforcement learning}},
  url     = {https://github.com/Farama-Foundation/PettingZoo},
  year    = {2021}
}

@article{AI2021110098,
  title    = {Coverage path planning for maritime search and rescue using reinforcement learning},
  journal  = {Ocean Engineering},
  volume   = {241},
  pages    = {110098},
  year     = {2021},
  issn     = {0029-8018},
  doi      = {10.1016/j.oceaneng.2021.110098},
  url      = {https://www.sciencedirect.com/science/article/pii/S0029801821014220},
  author   = {Bo Ai and Maoxin Jia and Hanwen Xu and Jiangling Xu and Zhen Wen and Benshuai Li and Dan Zhang},
  keywords = {Maritime search and rescue, Reinforcement learning, Coverage path planning, Intelligent planning},
  abstract = {In maritime search and rescue (SAR), the planning of the search path will directly affect the efficiency of searching for people overboard in the search area. However, traditional SAR decision-making schemes often adopt a fixed search path planning mode, but the limits are poor flexibility, low efficiency, and insufficient intelligence. This paper plans a search path with the shortest time-consuming and priority coverage of high-probability areas, considering complete coverage of maritime SAR areas and avoiding maritime obstacles. Firstly, a maritime SAR environment model is built using marine environmental field data and electronic charts. Secondly, an autonomous coverage path planning model for maritime SAR is proposed based on reinforcement learning, in which a reward function with multiple constraints is designed to guide the navigation action of the vessel agent. In the iterative training process of the path planning model, the random action selection probability is dynamically adjusted by the nonlinear action selection policy to ensure the stable convergence of the model. Finally, the experimental verification is conducted in different small-scale maritime SAR simulation scenarios. The results indicate that the search path can cover the high-probability areas preferentially with lower repeated coverage and shorter path length compared with other path planning algorithms.}
}

@article{WU2024116403,
  title    = {An autonomous coverage path planning algorithm for maritime search and rescue of persons-in-water based on deep reinforcement learning},
  journal  = {Ocean Engineering},
  volume   = {291},
  pages    = {116403},
  year     = {2024},
  issn     = {0029-8018},
  doi      = {10.1016/j.oceaneng.2023.116403},
  url      = {https://www.sciencedirect.com/science/article/pii/S0029801823027877},
  author   = {Jie Wu and Liang Cheng and Sensen Chu and Yanjie Song},
  keywords = {Persons-in-water, Drift trajectory prediction, Maritime search and rescue, Coverage path planning, Deep reinforcement learning},
  abstract = {The prevalence of maritime transportation and operations is increasing, leading to a gradual increase in drowning accidents at sea. In the context of maritime search and rescue (SAR), it is essential to develop effective search plans to improve the survival probability of persons-in-water (PIWs). However, conventional SAR search plans typically use predetermined patterns to ensure complete coverage of the search area, disregarding the varying probabilities associated with the PIW distribution. To address this issue, this study has proposed a maritime SAR vessel coverage path planning framework (SARCPPF) suitable for multiple PIWs. This framework comprises three modules, namely, drift trajectory prediction, the establishment of a multilevel search area environment model, and coverage search. First, sea area-scale drift trajectory prediction models were employed using the random particle simulation method to forecast drift trajectories. A hierarchical probability environment map model was established to guide the SAR of multiple SAR units. Subsequently, we integrated deep reinforcement learning with a reward function that encompasses multiple variables to guide the navigation behavior of ship agents. We developed a coverage path planning algorithm aimed at maximizing the success rates within a limited timeframe. The experimental results have demonstrated that our model enables vessel agents to prioritize high-probability regions while avoiding repeated coverage.}
}

@article{SILVER2021103535,
    title          = {Reward is enough},
    journal        = {Artificial Intelligence},
    volume         = {299},
    pages          = {103535},
    year           = {2021},
    issn           = {0004-3702},
    doi            = {https://doi.org/10.1016/j.artint.2021.103535},
    url            = {https://www.sciencedirect.com/science/article/pii/S0004370221000862},
    author         = {David Silver and Satinder Singh and Doina Precup and Richard S. Sutton},
    keywords       = {Artificial intelligence, Artificial general intelligence, Reinforcement learning, Reward},
    abstract       = {In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.}
}

@article{gmd-11-1405-2018,
  author  = {Dagestad, K.-F. and R\"ohrs, J. and Breivik, {\O}. and {\AA}dlandsvik, B.},
  title   = {OpenDrift v1.0: a generic framework for trajectory modelling},
  journal = {Geoscientific Model Development},
  volume  = {11},
  year    = {2018},
  number  = {4},
  pages   = {1405--1420},
  url     = {https://gmd.copernicus.org/articles/11/1405/2018/},
  doi     = {10.5194/gmd-11-1405-2018}
}

@conference{dsse2023,
  author       = {Leonardo D. M. de Abreu and Luis F. S. Carrete and Manuel Castanares and Enrico F. Damiani and José Fernando B. Brancalion and Fabrício J. Barth},
  title        = {Exploration and Rescue of Shipwreck Survivors using Reinforcement Learning-Empowered Drone Swarms},
  organization = {Simpósio de Aplicações Operacionais em Áreas de Defesa (SIGE)},
  year         = {2023},
  pages        = {64--69},
  address      = {São José dos Campos, SP},
  issn         = {1983-7402}
}

@article{dqn2015,
  title    = {Human-level control through deep reinforcement learning},
  author   = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and
              Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves,
              Alex and Riedmiller, Martin and Fidjeland, Andreas K and
              Ostrovski, Georg and Petersen, Stig and Beattie, Charles and
              Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran,
              Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  abstract = {An artificial agent is developed that learns to play a diverse
              range of classic Atari 2600 computer games directly from sensory
              experience, achieving a performance comparable to that of an
              expert human player; this work paves the way to building
              general-purpose learning algorithms that bridge the divide
              between perception and action.},
  journal  = {Nature},
  volume   = {518},
  number   = {7540},
  pages    = {529--533},
  month    = {feb},
  year     = {2015},
  doi      = {10.1038/nature14236}
}

@misc{ppo2017,
  author = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  title  = {Proximal Policy Optimization Algorithms},
  year   = {2017},
  eprint = {arXiv:1707.06347},
  doi    = {10.48550/arXiv.1707.06347}
}

@article{WU2023113444,
  title    = {Modeling the leeway drift characteristics of persons-in-water at a sea-area scale in the seas of China},
  journal  = {Ocean Engineering},
  volume   = {270},
  pages    = {113444},
  year     = {2023},
  issn     = {0029-8018},
  doi      = {10.1016/j.oceaneng.2022.113444},
  url      = {https://www.sciencedirect.com/science/article/pii/S0029801822027275},
  author   = {Jie Wu and Liang Cheng and Sensen Chu},
  keywords = {Persons-in-water, Drift characteristics, Sea-area-scale models, Probability of +CWL, Search and rescue},
  abstract = {Drowning events are becoming more common. It is important to accurately predict the drift trajectory of floating objects. This study proposes a sea-area-scale drift modeling method to determine the drift trajectory of persons-in-water in offshore China. The proposed method divided offshore China into 18 areas, and field experiments were carried out in the northern East China Sea (N_ES) and the Taiwan Strait (TS). First, a series of models were created, namely N_ES_I (N_ES upright), N_ES_II (N_ES facedown), TS_I (TS upright), TS_II (TS facedown), C_I (comprehensive upright), and C_II (comprehensive facedown). The leeway coefficients and jibing frequencies were then determined, and the positive crosswind speed (+CWL) probabilities under different marine environments were analyzed. A large volume of information from open-source channels was used to establish a database of historical drift cases. The driven model was validated using a combination of field experiments and historical drift information. The results indicate that the trajectories simulated using the sea-area-scale models proposed in this study are more consistent with the actual trajectories than those using the constant coefficients. The prediction areas are sensitive to the +CWL probability and the jibing frequencies. This study can help to improve the ability of the drift trajectory in China.}
}
